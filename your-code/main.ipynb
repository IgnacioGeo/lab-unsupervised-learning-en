{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data: Wholesale customers data\n",
    "import requests\n",
    "\n",
    "# Load the dataset\n",
    "path='C:/Users/Ignacio/IronHackCodes/gitHStuff/Labs/lab-unsupervised-learning-en-main/data/Wholesale customers data.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(df.head())# Display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "print(df.info())# Display basic information about the dataset\n",
    "\n",
    "#Any categorical data to convert?\n",
    "df['Channel'] = df['Channel'].astype('category')\n",
    "df['Region'] = df['Region'].astype('category')\n",
    "\n",
    "#Column collinearity - any high correlations?\n",
    "\n",
    "corr_matrix = df.corr()# Compute the correlation matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "#Descriptive statistics - any outliers to remove?\n",
    "\n",
    "print(df.describe())# Display descriptive statistics\n",
    "\n",
    "# Boxplots to detect outliers\n",
    "df.boxplot(figsize=(12, 8))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#Column-wise data distribution - is the distribution skewed?\n",
    "\n",
    "# Plot histograms to check for distribution\n",
    "df.hist(figsize=(12, 10), bins=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Pareto principle - does this dataset display this characteristic?\n",
    "\n",
    "# Calculate total spending for each customer\n",
    "df['Total_Spending'] = df.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "# Sort customers by total spending\n",
    "df_sorted = df.sort_values(by='Total_Spending', ascending=False)\n",
    "\n",
    "# Calculate cumulative spending and percentage of customers\n",
    "df_sorted['Cumulative_Spending'] = df_sorted['Total_Spending'].cumsum()\n",
    "df_sorted['Cumulative_Percentage'] = df_sorted['Cumulative_Spending'] / df_sorted['Total_Spending'].sum() * 100\n",
    "df_sorted['Customer_Percentage'] = (df_sorted.index + 1) / df_sorted.shape[0] * 100\n",
    "\n",
    "# Plot the cumulative distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_sorted['Customer_Percentage'], df_sorted['Cumulative_Percentage'], marker='o')\n",
    "plt.axhline(80, color='r', linestyle='--')\n",
    "plt.axvline(20, color='r', linestyle='--')\n",
    "plt.xlabel('Percentage of Customers')\n",
    "plt.ylabel('Percentage of Total Spending')\n",
    "plt.title('Pareto Principle (80/20 Rule)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "- Channel: Type of customer (1 = Horeca (Hotel/Restaurant/Cafe), 2 = Retail channel)\n",
    "- Region: Customer's region (1 = Lisbon, 2 = Oporto, 3 = Other regions)\n",
    "- Fresh: Annual spending on fresh products\n",
    "- Milk: Annual spending on milk products\n",
    "- Grocery: Annual spending on grocery products\n",
    "- Frozen: Annual spending on frozen products\n",
    "- Detergents_Paper: Annual spending on detergents and paper products\n",
    "- Delicassen: Annual spending on delicatessen products\n",
    "- There are no missing values in the dataset as indicated by the output of .info() where all columns have 440 non-null entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#Handling Outliers\n",
    "# Select only numerical columns for IQR calculation, excluding 'Channel' and 'Region'\n",
    "numerical_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate IQR for each numerical column\n",
    "Q1 = numerical_df.quantile(0.25)\n",
    "Q3 = numerical_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out outliers\n",
    "df_outliers_removed = numerical_df[~((numerical_df < (Q1 - 1.5 * IQR)) | (numerical_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "print(f\"Number of entries before removing outliers: {df.shape[0]}\")\n",
    "print(f\"Number of entries after removing outliers: {df_outliers_removed.shape[0]}\")\n",
    "\n",
    "# Add categorical columns back to the cleaned DataFrame\n",
    "df_outliers_removed['Channel'] = df['Channel']\n",
    "df_outliers_removed['Region'] = df['Region']\n",
    "\n",
    "\n",
    "#Standardization of Numerical Features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecting numerical features\n",
    "numerical_features = df_outliers_removed.select_dtypes(include=['int64', 'float64']).drop(['Channel', 'Region'], axis=1, errors='ignore')\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Converting back to DataFrame\n",
    "df_standardized = pd.DataFrame(df_standardized, columns=numerical_features.columns)\n",
    "\n",
    "# Adding 'Channel' and 'Region' columns back to the standardized DataFrame\n",
    "df_standardized['Channel'] = df_outliers_removed['Channel'].values\n",
    "df_standardized['Region'] = df_outliers_removed['Region'].values\n",
    "\n",
    "print(df_standardized.head())\n",
    "\n",
    "print(df_standardized.info())\n",
    "\n",
    "print(df_standardized.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "-  ...\n",
    "-  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your import here:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "numerical_features = df_outliers_removed.select_dtypes(include=['int64', 'float64']).drop(['Channel', 'Region'], axis=1, errors='ignore')# Ensure numerical features are selected, excluding 'Channel' and 'Region'\n",
    "\n",
    "scaler = StandardScaler()# Initialize the StandardScaler\n",
    "\n",
    "customers_scale = scaler.fit_transform(numerical_features)# Fit and transform the data\n",
    "\n",
    "customers_scale = pd.DataFrame(customers_scale, columns=numerical_features.columns)# Convert the scaled data back to a DataFrame\n",
    "\n",
    "# Add 'Channel' and 'Region' columns back to the scaled DataFrame\n",
    "customers_scale['Channel'] = df_outliers_removed['Channel'].values\n",
    "customers_scale['Region'] = df_outliers_removed['Region'].values\n",
    "\n",
    "print(customers_scale.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "n_clusters = 3# Define the number of clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, random_state=42)# Initialize the KMeans model\n",
    "\n",
    "kmeans.fit(customers_scale.drop(['Channel', 'Region'], axis=1))# Fit the model to the scaled data\n",
    "\n",
    "labels = kmeans.labels_# Extract cluster labels\n",
    "\n",
    "customers_scale['labels'] = labels# Assign cluster labels to the original data\n",
    "\n",
    "print(customers_scale.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2, init='k-means++', n_init=10, random_state=42).fit(customers_scale.drop(['Channel', 'Region'], axis=1))# Fit KMeans with 2 clusters\n",
    "\n",
    "labels = kmeans_2.predict(customers_scale.drop(['Channel', 'Region'], axis=1))# Predict the cluster labels\n",
    "\n",
    "customers_scale['Label'] = labels# Add the cluster labels to the DataFrame\n",
    "\n",
    "print(customers_scale.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_customers['Label'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Count the values in labels\n",
    "label_counts = customers_scale['Label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# Your code here\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5)# Initialize the DBSCAN model with eps=0.5\n",
    "\n",
    "dbscan.fit(customers_scale.drop(['Channel', 'Region', 'Label'], axis=1))# Fit the model to the scaled data\n",
    "\n",
    "labels_dbscan = dbscan.labels_# Extract cluster labels\n",
    "\n",
    "customers_scale['labels_DBSCAN'] = labels_dbscan# Assign DBSCAN cluster labels to the original data\n",
    "\n",
    "print(customers_scale.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Count the values in labels_DBSCAN\n",
    "label_counts_dbscan = customers_scale['labels_DBSCAN'].value_counts()\n",
    "print(label_counts_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, hue, title):\n",
    "    sns.scatterplot(x=x, y=y, hue=hue, palette='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.legend(title=hue.name, loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Plot Detergents_Paper vs Milk using K-Means labels\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot(x=customers_scale['Detergents_Paper'], y=customers_scale['Milk'], hue=customers_scale['Label'], title='K-Means: Detergents_Paper vs Milk')\n",
    "\n",
    "# Plot Detergents_Paper vs Milk using DBSCAN labels\n",
    "plt.subplot(1, 2, 2)\n",
    "plot(x=customers_scale['Detergents_Paper'], y=customers_scale['Milk'], hue=customers_scale['labels_DBSCAN'], title='DBSCAN: Detergents_Paper vs Milk')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Plot Grocery vs Fresh using K-Means labels\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot(x=customers_scale['Grocery'], y=customers_scale['Fresh'], hue=customers_scale['Label'], title='K-Means: Grocery vs Fresh')\n",
    "\n",
    "# Plot Grocery vs Fresh using DBSCAN labels\n",
    "plt.subplot(1, 2, 2)\n",
    "plot(x=customers_scale['Grocery'], y=customers_scale['Fresh'], hue=customers_scale['labels_DBSCAN'], title='DBSCAN: Grocery vs Fresh')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Plot Frozen vs Delicassen using K-Means labels\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot(x=customers_scale['Frozen'], y=customers_scale['Delicassen'], hue=customers_scale['Label'], title='K-Means: Frozen vs Delicassen')\n",
    "\n",
    "# Plot Frozen vs Delicassen using DBSCAN labels\n",
    "plt.subplot(1, 2, 2)\n",
    "plot(x=customers_scale['Frozen'], y=customers_scale['Delicassen'], hue=customers_scale['labels_DBSCAN'], title='DBSCAN: Frozen vs Delicassen')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Group by K-Means labels and compute the means\n",
    "kmeans_grouped = customers_scale.drop(['Channel', 'Region'], axis=1).groupby('Label').mean()\n",
    "print(\"Means of each column grouped by K-Means labels:\")\n",
    "print(kmeans_grouped)\n",
    "\n",
    "# Group by DBSCAN labels and compute the means\n",
    "dbscan_grouped = customers_scale.drop(['Channel', 'Region'], axis=1).groupby('labels_DBSCAN').mean()\n",
    "print(\"\\nMeans of each column grouped by DBSCAN labels:\")\n",
    "print(dbscan_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_clusters(data, x_col, y_col, labels, title):\n",
    "    sns.scatterplot(x=data[x_col], y=data[y_col], hue=labels, palette='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Experiment with 2, 3, 4, and 5 clusters\n",
    "n_clusters_list = [2, 3, 4, 5]\n",
    "\n",
    "for n_clusters in n_clusters_list:\n",
    "    # Initialize and fit KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, random_state=42)\n",
    "    kmeans.fit(customers_scale.drop(['Channel', 'Region', 'labels_DBSCAN'], axis=1))\n",
    "    \n",
    "    # Predict the cluster labels\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Plot the clusters\n",
    "    print(f\"Visualizing clusters for n_clusters = {n_clusters}\")\n",
    "    \n",
    "    # Detergents_Paper vs Milk\n",
    "    plot_clusters(customers_scale, 'Detergents_Paper', 'Milk', labels, f'K-Means: Detergents_Paper vs Milk (n_clusters={n_clusters})')\n",
    "    \n",
    "    # Grocery vs Fresh\n",
    "    plot_clusters(customers_scale, 'Grocery', 'Fresh', labels, f'K-Means: Grocery vs Fresh (n_clusters={n_clusters})')\n",
    "    \n",
    "    # Frozen vs Delicassen\n",
    "    plot_clusters(customers_scale, 'Frozen', 'Delicassen', labels, f'K-Means: Frozen vs Delicassen (n_clusters={n_clusters})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_clusters(data, x_col, y_col, labels, title):\n",
    "    sns.scatterplot(x=data[x_col], y=data[y_col], hue=labels, palette='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# List of (eps, min_samples) pairs to experiment with\n",
    "params_list = [\n",
    "    (0.3, 5),\n",
    "    (0.5, 5),\n",
    "    (0.5, 10),\n",
    "    (0.7, 10)\n",
    "]\n",
    "\n",
    "for eps, min_samples in params_list:\n",
    "    # Initialize and fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(customers_scale.drop(['Channel', 'Region', 'Label'], axis=1))\n",
    "    \n",
    "    # Predict the cluster labels\n",
    "    labels_dbscan = dbscan.labels_\n",
    "    \n",
    "    # Plot the clusters\n",
    "    print(f\"Visualizing clusters for DBSCAN with eps = {eps}, min_samples = {min_samples}\")\n",
    "    \n",
    "    # Detergents_Paper vs Milk\n",
    "    plot_clusters(customers_scale, 'Detergents_Paper', 'Milk', labels_dbscan, f'DBSCAN: Detergents_Paper vs Milk (eps={eps}, min_samples={min_samples})')\n",
    "    \n",
    "    # Grocery vs Fresh\n",
    "    plot_clusters(customers_scale, 'Grocery', 'Fresh', labels_dbscan, f'DBSCAN: Grocery vs Fresh (eps={eps}, min_samples={min_samples})')\n",
    "    \n",
    "    # Frozen vs Delicassen\n",
    "    plot_clusters(customers_scale, 'Frozen', 'Delicassen', labels_dbscan, f'DBSCAN: Frozen vs Delicassen (eps={eps}, min_samples={min_samples})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
